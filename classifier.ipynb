{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e677abdf",
   "metadata": {},
   "source": [
    "***Image pre-processing (True Color, False Color, NDVI)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872fe7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of NDVI Image\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Import images as numpy arrays\n",
    "b04_img = Image.open('images/b04.jpg')\n",
    "b08_img = Image.open('images/b08.jpg')\n",
    "\n",
    "b04_array = np.array(b04_img)\n",
    "b08_array = np.array(b08_img)\n",
    "\n",
    "# Normalize the arrays to float32\n",
    "b04_array = b04_array.astype(np.float32) / 255.0\n",
    "b08_array = b08_array.astype(np.float32) / 255.0\n",
    "\n",
    "# Calculate NDVI\n",
    "ndvi = (b08_array - b04_array) / (b08_array + b04_array)\n",
    "\n",
    "# Map NDVI values to 0-255 for visualization\n",
    "ndvi_mapped = (((ndvi + 1) / 2)* 255).astype(np.uint8)\n",
    "\n",
    "# Create RGB array (ndvi, ndvi, ndvi)\n",
    "ndvi_rgb = np.stack((ndvi_mapped, ndvi_mapped, ndvi_mapped), axis=-1)\n",
    "\n",
    "# Convert to PIL Image\n",
    "ndvi_image = Image.fromarray(ndvi_rgb)\n",
    "\n",
    "# Convert into 512x512\n",
    "ndvi_image = ndvi_image.resize((512, 512), Image.LANCZOS)\n",
    "\n",
    "# Save the NDVI image\n",
    "ndvi_image.save('images/ndvi.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a59a07",
   "metadata": {},
   "source": [
    "***Model import***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aa785aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1932450fe2dd4f31835b21c303297b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ControlNetModel(\n",
      "  (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (time_proj): Timesteps()\n",
      "  (time_embedding): TimestepEmbedding(\n",
      "    (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "    (act): SiLU()\n",
      "    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (controlnet_cond_embedding): ControlNetConditioningEmbedding(\n",
      "    (conv_in): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (blocks): ModuleList(\n",
      "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): Conv2d(32, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (4): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): Conv2d(96, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (conv_out): Conv2d(256, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (down_blocks): ModuleList(\n",
      "    (0): CrossAttnDownBlock2D(\n",
      "      (attentions): ModuleList(\n",
      "        (0-1): 2 x Transformer2DModel(\n",
      "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "          (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (transformer_blocks): ModuleList(\n",
      "            (0): BasicTransformerBlock(\n",
      "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn1): Attention(\n",
      "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
      "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn2): Attention(\n",
      "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "                (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
      "                (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (resnets): ModuleList(\n",
      "        (0-1): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
      "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (downsamplers): ModuleList(\n",
      "        (0): Downsample2D(\n",
      "          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): CrossAttnDownBlock2D(\n",
      "      (attentions): ModuleList(\n",
      "        (0-1): 2 x Transformer2DModel(\n",
      "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "          (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (transformer_blocks): ModuleList(\n",
      "            (0): BasicTransformerBlock(\n",
      "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn1): Attention(\n",
      "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
      "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn2): Attention(\n",
      "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "                (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
      "                (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (resnets): ModuleList(\n",
      "        (0): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
      "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
      "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (downsamplers): ModuleList(\n",
      "        (0): Downsample2D(\n",
      "          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): CrossAttnDownBlock2D(\n",
      "      (attentions): ModuleList(\n",
      "        (0-1): 2 x Transformer2DModel(\n",
      "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (transformer_blocks): ModuleList(\n",
      "            (0): BasicTransformerBlock(\n",
      "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn1): Attention(\n",
      "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn2): Attention(\n",
      "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
      "                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (resnets): ModuleList(\n",
      "        (0): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (downsamplers): ModuleList(\n",
      "        (0): Downsample2D(\n",
      "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): DownBlock2D(\n",
      "      (resnets): ModuleList(\n",
      "        (0-1): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (controlnet_down_blocks): ModuleList(\n",
      "    (0-3): 4 x Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (4-6): 3 x Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (7-11): 5 x Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (controlnet_mid_block): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (mid_block): UNetMidBlock2DCrossAttn(\n",
      "    (attentions): ModuleList(\n",
      "      (0): Transformer2DModel(\n",
      "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (transformer_blocks): ModuleList(\n",
      "          (0): BasicTransformerBlock(\n",
      "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn1): Attention(\n",
      "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "              (to_out): ModuleList(\n",
      "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn2): Attention(\n",
      "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
      "              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
      "              (to_out): ModuleList(\n",
      "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (ff): FeedForward(\n",
      "              (net): ModuleList(\n",
      "                (0): GEGLU(\n",
      "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
      "                )\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (resnets): ModuleList(\n",
      "      (0-1): 2 x ResnetBlock2D(\n",
      "        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "        (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from diffusers import ControlNetModel, StableDiffusionControlNetPipeline\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\"mespinosami/sen12mscr-sd-1_5\")\n",
    "base_model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "\tbase_model_id, controlnet=controlnet\n",
    ")\n",
    "\n",
    "print(controlnet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9095d9c",
   "metadata": {},
   "source": [
    "***Input images*** \n",
    "Pre processing of all the three different images and input in the model without touching the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04763a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_color = Image.open('images/true_color.jpg')\n",
    "false_color = Image.open('images/false_color.jpg')\n",
    "ndvi_image = Image.open('images/ndvi.jpg')\n",
    "\n",
    "# Convert into 512x512\n",
    "true_color = true_color.resize((512, 512), Image.LANCZOS)\n",
    "false_color = false_color.resize((512, 512), Image.LANCZOS)\n",
    "\n",
    "\n",
    "# Print the sizes of the images\n",
    "print(f\"True Color Image Size: {true_color.size}\")\n",
    "print(f\"False Color Image Size: {false_color.size}\")\n",
    "print(f\"NDVI Image Size: {ndvi_image.size}\")\n",
    "\n",
    "# Print mode checks\n",
    "print(f\"True Color Image Mode: {true_color.mode}\")\n",
    "print(f\"False Color Image Mode: {false_color.mode}\")\n",
    "print(f\"NDVI Image Mode: {ndvi_image.mode}\")\n",
    "\n",
    "# Pipeline execution\n",
    "prompt = 'a satellite view of some agricultural fields, in high-resolution, top-down view, near Rome'\n",
    "controlnet_image = 'true_color'  # Options: 'true_color', 'false_color', 'ndvi'\n",
    "\n",
    "output = pipe(\n",
    "    prompt=prompt,\n",
    "    image=ndvi_image,\n",
    "    num_inference_steps=50\n",
    ")\n",
    "\n",
    "output_image = output.images[0]\n",
    "# Save the output image\n",
    "output_image.save(f'images/output_image' + '_ndvi_image' + '.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623fbf1",
   "metadata": {},
   "source": [
    "***Phase 2***\n",
    "Fine-Tuning on the last 2 model layers\n",
    "Image pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a93992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre processing of the data segmenting each image and mask into patches of size 512x512\n",
    "# and saving them into the respective folders for training and validation\n",
    "import os\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "import rasterio as rio\n",
    "import cv2\n",
    "\n",
    "IMAGE_PATH =  'dataset/images/not_converted'\n",
    "MASK_PATH = 'dataset/masks/not_converted'\n",
    "\n",
    "# Read the image\n",
    "def read_image(image_path):\n",
    "    with rio.open(image_path) as src:\n",
    "        image = src.read()\n",
    "        image = image.astype(np.uint8)\n",
    "    return image\n",
    "\n",
    "# Read the mask\n",
    "def read_mask(mask_path):\n",
    "    with rio.open(mask_path) as src:\n",
    "        mask = src.read()\n",
    "        mask = np.squeeze(mask, axis=0)\n",
    "        # Upscale from 20m to 10m with nearest neighbor interpolation\n",
    "        mask = cv2.resize(mask, (0, 0), fx=2, fy=2, interpolation=cv2.INTER_NEAREST)\n",
    "        mask = mask.astype(np.uint8)\n",
    "    return mask\n",
    "\n",
    "image_files = sorted(f for f in os.listdir(IMAGE_PATH) if f.endswith('.jp2'))\n",
    "mask_files = sorted(f for f in os.listdir(MASK_PATH) if f.endswith('.jp2'))\n",
    "patch_size = 512\n",
    "stride = 512\n",
    "\n",
    "for image_file, mask_file in zip(image_files, mask_files):\n",
    "    image_path = os.path.join(IMAGE_PATH, image_file)\n",
    "    mask_path = os.path.join(MASK_PATH, mask_file)\n",
    "\n",
    "    # Read the image and mask\n",
    "    image = read_image(image_path)\n",
    "    mask = read_mask(mask_path)\n",
    "    \n",
    "    height, witdth = image.shape[1:]\n",
    "    \n",
    "    # Slide windoew over the image to get windows of size 512x512\n",
    "    for y in range(0, height - patch_size + 1, stride):\n",
    "        for x in range(0, witdth - patch_size + 1, stride):\n",
    "            image_patch = image[:, y:y+512, x:x+512]\n",
    "            mask_patch = mask[y:y+512, x:x+512]\n",
    "            # Save image and mask patches\n",
    "            image_patch_path = os.path.join('dataset/images/train', f'{image_file[:-4]}_{y}_{x}.png')\n",
    "            mask_patch_path = os.path.join('dataset/masks/val', f'{mask_file[:-4]}_{y}_{x}.png')\n",
    "            image_patch_image = Image.fromarray(image_patch.transpose(1, 2, 0)) # Transpose from (C, H, W) to (H, W, C)\n",
    "            mask_patch_image = Image.fromarray(mask_patch.astype(np.uint8), mode='L')\n",
    "            image_patch_image.save(image_patch_path)\n",
    "            mask_patch_image.save(mask_patch_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78841e8a",
   "metadata": {},
   "source": [
    "***Fine Tuning***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e461a7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Explore the ControlNetModel to choose which blocks to train\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Dummy inputs\n",
    "sample = torch.randn(1, 4, 64, 64).to(device)                    # latent\n",
    "timestep = torch.tensor([50]).to(device)                            # e.g. 50\n",
    "encoder_hidden_states = torch.randn(1, 77, 768).to(device)           # from tokenizer/CLIP\n",
    "controlnet_cond = torch.randn(1, 3, 512, 512).to(device)    # e.g. RGB/NDVI\n",
    "\n",
    "controlnet = controlnet.to(device)\n",
    "\n",
    "# Forward pass like:\n",
    "controlnet(sample, timestep, encoder_hidden_states, controlnet_cond)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ed6e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256]) torch.Size([1, 3, 256, 256])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d699599ff2a4898bda6a12196189191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5656 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "  0%|          | 0/5656 [00:20<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.87 GiB is allocated by PyTorch, and 122.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 182\u001b[0m\n\u001b[0;32m    180\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(noise_pred, noise)\n\u001b[0;32m    181\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 182\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    184\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\anaconda3\\envs\\tesi\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:137\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[0;32m    136\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(opt, opt\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\anaconda3\\envs\\tesi\\lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\anaconda3\\envs\\tesi\\lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\anaconda3\\envs\\tesi\\lib\\site-packages\\torch\\optim\\adamw.py:209\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    206\u001b[0m     amsgrad: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    207\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m--> 209\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     adamw(\n\u001b[0;32m    221\u001b[0m         params_with_grad,\n\u001b[0;32m    222\u001b[0m         grads,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    240\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    241\u001b[0m     )\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\anaconda3\\envs\\tesi\\lib\\site-packages\\torch\\optim\\adamw.py:148\u001b[0m, in \u001b[0;36mAdamW._init_group\u001b[1;34m(self, group, params_with_grad, grads, amsgrad, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[0;32m    138\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    139\u001b[0m     torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[0;32m    140\u001b[0m         (),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m_get_scalar_dtype())\n\u001b[0;32m    146\u001b[0m )\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# Exponential moving average of gradient values\u001b[39;00m\n\u001b[1;32m--> 148\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[0;32m    152\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\n\u001b[0;32m    153\u001b[0m     p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format\n\u001b[0;32m    154\u001b[0m )\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.87 GiB is allocated by PyTorch, and 122.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionControlNetPipeline, DDPMScheduler, AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
    "\n",
    "# DataLoader for training\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os \n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode as Im\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "# Warning handling\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for name, param in controlnet.named_parameters():\n",
    "    if name.startswith(\"mid_block.\") or name.startswith(\"down_blocks.3.\"):\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "class imageDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])\n",
    "        self.mask_files = sorted([f for f in os.listdir(mask_dir) if f.endswith('.png')])\n",
    "        \n",
    "        # Transform images: resize to 512x512, normalize, and convert to tensor\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256), interpolation=Im.BILINEAR),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        \n",
    "        # Transform masks: resize to 512x512 and convert to tensor\n",
    "        self.mask_transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256), interpolation=Im.NEAREST),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_files[idx])\n",
    "        \n",
    "        # Open images and masks\n",
    "        image = Image.open(image_path).convert('RGB')  # Ensure image is in RGB format\n",
    "        mask = Image.open(mask_path).convert('RGB')  # Ensure mask is in grayscale format\n",
    "        \n",
    "        image = self.image_transform(image)\n",
    "        mask = self.mask_transform(mask)\n",
    "        # mask = (mask * 255).long()  # Convert mask to long tensor for compatibility with loss functions (integer class labels)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "train_dataset = imageDataset(\n",
    "    image_dir='dataset/train/images',\n",
    "    mask_dir='dataset/train/masks'\n",
    ")\n",
    "\n",
    "val_dataset = imageDataset(\n",
    "    image_dir='dataset/val/images',\n",
    "    mask_dir='dataset/val/masks'\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    break\n",
    "\n",
    "# Initialize the Stable Diffusion pipeline with ControlNet\n",
    "base_model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    base_model_id,\n",
    "    controlnet=controlnet,\n",
    ")\n",
    "\n",
    "pipe.to(device)\n",
    "# pipe.enable_model_cpu_offload()\n",
    "\n",
    "\n",
    "# Optimizer needs to select only the trainabe parameters\n",
    "parmas_to_train =[]\n",
    "for name, param in controlnet.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        parmas_to_train.append(param)\n",
    "\n",
    "optimizer = AdamW(parmas_to_train, lr=1e-5)  # 1e-5, \n",
    "\n",
    "vae = pipe.vae                      # Code/Decode Images\n",
    "unet = pipe.unet                    # To train the model  \n",
    "text_encoder = pipe.text_encoder    # Encode text prompts\n",
    "tokenizer = pipe.tokenizer          # Tokenizer for text prompts\n",
    "\n",
    "for param in vae.parameters():\n",
    "    param.requires_grad = False  # Freeze VAE parameters\n",
    "for param in text_encoder.parameters():\n",
    "    param.requires_grad = False  # Freeze UNet parameters\n",
    "\n",
    "noise_scheduler = PNDMScheduler(\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    num_train_timesteps=1000,\n",
    "    skip_prk_steps=True,\n",
    "    steps_offset=1,\n",
    "    set_alpha_to_one=False\n",
    ") # Noise scheduler for training\n",
    "\n",
    "# On GPU\n",
    "# vae.to(device)\n",
    "# unet.to(device)\n",
    "# text_encoder.to(device)\n",
    "unet.enable_gradient_checkpointing()\n",
    "\n",
    "num_epochs = 5\n",
    "total_steps = num_epochs * len(train_loader)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    controlnet.train()\n",
    "    unet.train()\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(train_loader)):\n",
    "        images, masks = batch\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        latents = vae.encode(images).latent_dist.sample()\n",
    "        latents = latents * vae.config.scaling_factor\n",
    "        \n",
    "        noise = torch.randn_like(latents)\n",
    "        timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (latents.shape[0],), device=latents.device).long()\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "        \n",
    "        prompt = [\"Satellite image\"] * latents.shape[0] # * batch size\n",
    "        input_ids = tokenizer(prompt, max_length=tokenizer.model_max_length, return_tensors=\"pt\").input_ids.to(device)\n",
    "        encoder_hidden_states = text_encoder(input_ids)[0]\n",
    "        \n",
    "        control_image = masks\n",
    "        \n",
    "        # Forward \n",
    "        down_block_res_samples, mid_block_res_sample = controlnet(\n",
    "            noisy_latents, timesteps, encoder_hidden_states, controlnet_cond=control_image, return_dict=False\n",
    "        )\n",
    "        \n",
    "        # Unet output with conditioning\n",
    "        noise_pred = unet(\n",
    "            sample=noisy_latents,\n",
    "            timestep=timesteps,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            down_block_additional_residuals=down_block_res_samples,\n",
    "            mid_block_additional_residual=mid_block_res_sample\n",
    "        ).sample\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Loss\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{step+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "            # Save weights every 10 epoch  \n",
    "            torch.save(controlnet.state_dict(), f'controlnet_epoch{epoch+1}.pth')\n",
    "        \n",
    "pipe.eval()\n",
    "val_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step, batch in enumerate(tqdm(val_loader)):\n",
    "        images, masks = batch\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        # Genera latenti e noisy come nel training\n",
    "        latents = vae.encode(images).latent_dist.sample() * vae.config.scaling_factor\n",
    "        noise = torch.randn_like(latents)\n",
    "        timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (latents.shape[0],), device=latents.device).long()\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        prompt = [\"Satellite image\"] * images.shape[0]\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "        encoder_hidden_states = text_encoder(input_ids)[0]\n",
    "\n",
    "        # Forward\n",
    "        down_block_res_samples, mid_block_res_sample = controlnet(\n",
    "            noisy_latents, timesteps, encoder_hidden_states, controlnet_cond=masks, return_dict=False\n",
    "        )\n",
    "        noise_pred = unet(\n",
    "            sample=noisy_latents,\n",
    "            timestep=timesteps,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            down_block_additional_residuals=down_block_res_samples,\n",
    "            mid_block_additional_residual=mid_block_res_sample\n",
    "        ).sample\n",
    "\n",
    "        val_loss += F.mse_loss(noise_pred, noise).item()\n",
    "\n",
    "avg_val_loss = val_loss / len(val_loader)\n",
    "print(f\"Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadcc80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights\n",
    "controlnet.load_state_dict(torch.load('controlnet_epoch5.pth'))\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    controlnet=controlnet\n",
    ").to(device)\n",
    "\n",
    "image = pipe(prompt=\"Satellite image\", controlnet_conditioning_image=mask_input_image).images[0]\n",
    "image.save(\"output.png\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
